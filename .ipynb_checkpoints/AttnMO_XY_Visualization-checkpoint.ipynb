{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate, Layer, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, Flatten,LeakyReLU,ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from gumbel_softmax_EJ import GumbelSoftmax\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "pd.options.display.max_rows = 2000\n",
    "\n",
    "\"\"\"\n",
    "## Implement multi head self attention as a Keras layer\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "## Implement a Transformer block as a layer\n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\"\"\"\n",
    "# Positional Embedding\n",
    "\"\"\"\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding1D, self).__init__()\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 3:\n",
    "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
    "        _, x, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1)\n",
    "        emb = torch.zeros((x,self.channels),device=tensor.device).type(tensor.type())\n",
    "        emb[:,:self.channels] = emb_x\n",
    "\n",
    "        return emb[None,:,:orig_ch]\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        channels = int(np.ceil(channels/2))\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "        _, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1)\n",
    "        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1)\n",
    "        emb = torch.zeros((x,y,self.channels*2),device=tensor.device).type(tensor.type())\n",
    "        emb[:,:,:self.channels] = emb_x\n",
    "        emb[:,:,self.channels:2*self.channels] = emb_y\n",
    "        return emb[None,:,:,:orig_ch]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "gpu = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device = gpu, enable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cond_R = pd.read_csv('01_Data/x_train_cond_R_Coord_ID_DisR.csv')\n",
    "y_train_cond_R = pd.read_csv('01_Data/y_train_cond_R_Coord_ID_DisR.csv')\n",
    "y_test_cond_SC_R = pd.read_csv('01_Data/y_test_cond_SC_R_Coord_ID_DisR.csv')\n",
    "y_train_cond_SC_R = pd.read_csv('01_Data/y_train_cond_SC_R_Coord_ID_DisR.csv')\n",
    "\n",
    "x_train_cond_R = x_train_cond_R[y_train_cond_R['Max_seq'] != 6]\n",
    "y_train_cond_R = y_train_cond_R[y_train_cond_R['Max_seq'] != 6]\n",
    "y_test_cond_SC_R = y_test_cond_SC_R[y_test_cond_SC_R['Max_seq'] != 6]\n",
    "y_train_cond_SC_R = y_train_cond_SC_R[y_train_cond_SC_R['Max_seq'] != 6]\n",
    "\n",
    "x_catcol = x_train_cond_R.columns.drop(['ID','P_Trip_seq'])\n",
    "y_catcol = y_train_cond_R.columns.drop(['ID','P_Trip_seq',\"JIGA\",\"P_Home_Meanage\",\"P_Home_Older\"])\n",
    "\n",
    "x_train_cond_R[x_catcol]= x_train_cond_R[x_catcol].apply(lambda x: x.astype('category') )\n",
    "y_train_cond_R[y_catcol]= y_train_cond_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "y_test_cond_SC_R[y_catcol]= y_test_cond_SC_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "y_train_cond_SC_R[y_catcol]= y_train_cond_SC_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "\n",
    "x_train_cond_R = x_train_cond_R.sort_values(by=['ID','P_Trip_seq'],axis=0)\n",
    "y_train_cond_R = y_train_cond_R.sort_values(by=['ID','P_Trip_seq'],axis=0)\n",
    "y_train_cond_SC_R = y_train_cond_SC_R.sort_values(by=['NID','P_Trip_seq'],axis=0)\n",
    "y_test_cond_SC_R = y_test_cond_SC_R.sort_values(by=['NID','P_Trip_seq'],axis=0)\n",
    "\n",
    "## Ground-truth of X\n",
    "\n",
    "samples = pd.concat([x_train_cond_R['ID'],x_train_cond_R[x_catcol].drop('P_Trip_purpose',axis=1)],axis=1)\n",
    "samples_R = x_train_cond_R.copy()\n",
    "samples_R['idx'] = samples_R.groupby('ID').cumcount()\n",
    "samples_R['prod_idx'] = 'TP_' + samples_R.idx.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Trip_purpose = samples_R.pivot(index='ID',columns='prod_idx',values='P_Trip_purpose')\n",
    "for col in Trip_purpose.columns:\n",
    "    Trip_purpose[col] = Trip_purpose[col].cat.add_categories(\"Z\").fillna(\"Z\")    \n",
    "\n",
    "Trip_purpose = Trip_purpose[Trip_purpose['TP_1'] != 'Z']    \n",
    "\n",
    "samples =  pd.merge(samples.groupby('ID').head(1),Trip_purpose,on=\"ID\")\n",
    "samples = samples.drop(['ID'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "## Processing the x_train_cond\n",
    "x_train_cond = pd.concat([x_train_cond_R['ID'],pd.get_dummies(x_train_cond_R[x_catcol].drop('P_Trip_purpose',axis=1))],axis=1)\n",
    "x_train_cond_R['idx'] = x_train_cond_R.groupby('ID').cumcount()\n",
    "x_train_cond_R['prod_idx'] = 'TP_' + x_train_cond_R.idx.astype(str)\n",
    "\n",
    "Trip_purpose = x_train_cond_R.pivot(index='ID',columns='prod_idx',values='P_Trip_purpose')\n",
    "for col in Trip_purpose.columns:\n",
    "    Trip_purpose[col] = Trip_purpose[col].cat.add_categories(\"Z\").fillna(\"Z\")\n",
    "\n",
    "Trip_purpose = Trip_purpose[Trip_purpose['TP_1'] != 'Z']    \n",
    "\n",
    "Trip_purpose = pd.get_dummies(Trip_purpose)\n",
    "x_train_cond =  pd.merge(x_train_cond.groupby('ID').head(1),Trip_purpose,on=\"ID\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train_cat = y_train_cond_R[['isHome','P_Arrival_time','stay_time','tr_time']]\n",
    "#y_train_num = y_train_cond_R[['Age_SC','start_time','stay_time','tr_time','JIGA','P_Home_Meanage','P_Home_Older']]\n",
    "y_train_seq = pd.concat([pd.get_dummies(y_train_cat),y_train_cond_R[['P_Arrival_x','P_Arrival_y','ID','P_Trip_seq']]],axis=1)\n",
    "y_train_seq = y_train_seq[y_train_seq['ID'].isin(x_train_cond['ID'])]\n",
    "\n",
    "\n",
    "y_train_SC_cat = y_train_cond_SC_R[['isHome','P_Arrival_time','stay_time','tr_time']]\n",
    "y_train_SC_seq = pd.concat([pd.get_dummies(y_train_SC_cat),y_train_cond_SC_R[['P_Arrival_x','P_Arrival_y','NID','P_Trip_seq']]],axis=1)\n",
    "\n",
    "y_train_nseq = pd.concat([pd.get_dummies(y_train_cond_R[['Age_SC','start_time']]),y_train_cond_R[['ID','JIGA','P_Home_Meanage','P_Home_Older']]],axis=1)\n",
    "y_train_SC_nseq = pd.concat([pd.get_dummies(y_train_cond_SC_R[['Age_SC','start_time']]),y_train_cond_SC_R[['NID','JIGA','P_Home_Meanage','P_Home_Older']]],axis=1)\n",
    "\n",
    "y_train_nseq = y_train_nseq.groupby('ID').head(1)\n",
    "y_train_nseq = y_train_nseq[y_train_nseq['ID'].isin(x_train_cond['ID'])]\n",
    "y_train_nseq = y_train_nseq.drop(['ID'],axis=1)\n",
    "\n",
    "y_train_SC_nseq = y_train_SC_nseq.groupby('NID').head(1)\n",
    "y_train_SC_nseq = y_train_SC_nseq.drop(['NID'],axis=1)\n",
    "\n",
    "x_train_cond = x_train_cond.drop(['ID'],axis=1)\n",
    "\n",
    "# ## Divide the x_train / x_test, y_train / y_test\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(x_train_cond, groups=x_train_cond.index))\n",
    "\n",
    "# x_test_cond,y_test_seq,y_test_nseq = x_train_cond.iloc[test_inds],y_train_seq.iloc[test_inds],y_train_nseq.iloc[test_inds] \n",
    "# x_train_cond,y_train_seq,y_train_nseq = x_train_cond.iloc[train_inds],y_train_seq.iloc[train_inds],y_train_nseq.iloc[train_inds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(y_train_seq.columns)-4\n",
    "maxlen = 5\n",
    "num_data = y_train_seq['ID'].nunique()\n",
    "num_data_SC = y_train_SC_seq['NID'].nunique()\n",
    "\n",
    "## Adding dummy dimension to be divded 4\n",
    "for i in range(3):\n",
    "    y_train_seq.insert(num_features,i,0)\n",
    "    y_train_SC_seq.insert(num_features,i,0)\n",
    "\n",
    "num_features = len(y_train_seq.columns)-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_SC_seq = np.load('01_Data/y_train_SC_seq.npy',allow_pickle=True)\n",
    "y_test_SC_seq = np.load('01_Data/y_test_SC_seq.npy',allow_pickle=True)\n",
    "y_train_seq = np.load('01_Data/y_train_seq.npy',allow_pickle=True)\n",
    "y_test_seq = np.load('01_Data/y_test_seq.npy',allow_pickle=True)\n",
    "\n",
    "y_train_nseq= pd.read_csv('01_Data/y_train_nseq.csv')\n",
    "y_test_nseq= pd.read_csv('01_Data/y_test_nseq.csv')\n",
    "y_train_SC_nseq= pd.read_csv('01_Data/y_train_SC_nseq.csv')\n",
    "y_test_SC_nseq= pd.read_csv('01_Data/y_test_SC_nseq.csv')\n",
    "x_train_cond= pd.read_csv('01_Data/x_train_cond.csv')\n",
    "x_test_cond= pd.read_csv('01_Data/x_test_cond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = y_train_seq\n",
    "# data_len = len(data)\n",
    "# np.where(data[:,:,51][:,3] != 0)\n",
    "\n",
    "## 2. Seq Emb (numpy Version)\n",
    "def seq(data, data_len):\n",
    "    \n",
    "    pos_encoding = PositionalEncoding1D(num_features)\n",
    "    d = torch.zeros((1,maxlen,num_features))\n",
    "    pos_1d_emb = pos_encoding(d) \n",
    "    pos_1d_emb = pos_1d_emb.numpy()\n",
    "    \n",
    "    pos_seq_emb = []\n",
    "    for i in range(data_len):\n",
    "        for j in range(maxlen):\n",
    "            a = int(data[i,j,num_features+3])\n",
    "            if a == 1 :\n",
    "                b = pos_1d_emb[:,0,:]\n",
    "            elif a == 2:\n",
    "                b = pos_1d_emb[:,1,:]\n",
    "            elif a == 3:\n",
    "                b = pos_1d_emb[:,2,:]\n",
    "            elif a == 4:\n",
    "                b = pos_1d_emb[:,3,:]\n",
    "            elif a == 5:\n",
    "                b = pos_1d_emb[:,4,:]\n",
    "            else :\n",
    "                b = np.zeros((1,num_features))\n",
    "            pos_seq_emb.append(b)\n",
    "    pos_seq_emb=np.array(pos_seq_emb)\n",
    "    pos_seq_emb=pos_seq_emb.reshape(data_len,maxlen,num_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "\n",
    "    \n",
    "    data_seq = data + pos_seq_emb\n",
    "    \n",
    "    return data_seq\n",
    "\n",
    "#%% 3. seq+loc Emb\n",
    "def selo(data, data_len):\n",
    "    pos_encoding = PositionalEncoding1D(num_features)\n",
    "    d = torch.zeros((1,maxlen,num_features))\n",
    "    pos_1d_emb = pos_encoding(d) \n",
    "    pos_1d_emb = pos_1d_emb.numpy()\n",
    "    \n",
    "    pos_seq_emb = []    \n",
    "    for i in range(data_len):\n",
    "        for j in range(maxlen):\n",
    "            a = int(data[i,j,num_features+3])\n",
    "            if a == 1 :\n",
    "                b = pos_1d_emb[:,0,:]\n",
    "            elif a == 2:\n",
    "                b = pos_1d_emb[:,1,:]\n",
    "            elif a == 3:\n",
    "                b = pos_1d_emb[:,2,:]\n",
    "            elif a == 4:\n",
    "                b = pos_1d_emb[:,3,:]\n",
    "            elif a == 5:\n",
    "                b = pos_1d_emb[:,4,:]\n",
    "            else :\n",
    "                b = np.zeros((1,num_features))\n",
    "            pos_seq_emb.append(b)\n",
    "    pos_seq_emb=np.array(pos_seq_emb)\n",
    "    pos_seq_emb=pos_seq_emb.reshape(data_len,maxlen,num_features)\n",
    "    \n",
    "    p_enc_2d = PositionalEncoding2D(num_features)\n",
    "    m = torch.zeros((1,95,40,num_features)) # 21 by 21 grids\n",
    "    pos_2d_emb = p_enc_2d(m)\n",
    "    pos_2d_emb = pos_2d_emb.numpy()\n",
    "    pos_2d_emb[:,0,0,:] = 0\n",
    "    \n",
    "    pos_loc_emb = []    \n",
    "    for i in range(data_len):           \n",
    "        a=pos_2d_emb[:,int(data[i,0,num_features]),int(data[i,0,num_features+1]),:]\n",
    "        b=pos_2d_emb[:,int(data[i,1,num_features]),int(data[i,1,num_features+1]),:]\n",
    "        c=pos_2d_emb[:,int(data[i,2,num_features]),int(data[i,2,num_features+1]),:]\n",
    "        d=pos_2d_emb[:,int(data[i,3,num_features]),int(data[i,3,num_features+1]),:]\n",
    "        e=pos_2d_emb[:,int(data[i,4,num_features]),int(data[i,4,num_features+1]),:]\n",
    "        pos_loc_emb.append(a)\n",
    "        pos_loc_emb.append(b)\n",
    "        pos_loc_emb.append(c)\n",
    "        pos_loc_emb.append(d)\n",
    "        pos_loc_emb.append(e)\n",
    "    pos_loc_emb=np.array(pos_loc_emb)\n",
    "    pos_loc_emb=np.reshape(pos_loc_emb, (data_len,maxlen,num_features))\n",
    "    \n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "    data = np.delete(data,num_features, axis=2)\n",
    "\n",
    "    data_seq_loc = data + pos_seq_emb + pos_loc_emb\n",
    "    \n",
    "    return data_seq_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq_1d = seq(y_train_seq,len(y_train_seq))\n",
    "y_train_seq_2d = selo(y_train_seq,len(y_train_seq))\n",
    "y_train_SC_seq_1d = seq(y_train_SC_seq,len(y_train_SC_seq))\n",
    "y_train_SC_seq_2d = selo(y_train_SC_seq,len(y_train_SC_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    noise = Input(shape=(latent_dim))\n",
    "    label_ns = Input(shape=(nseq_dim))  \n",
    "    label = Input(shape=(maxlen,embed_dim))\n",
    "    \n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(label)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    \n",
    "    k = Flatten()(x)   \n",
    "    \n",
    "    inputs = Concatenate()([noise,label_ns,k])  \n",
    "    \n",
    "    h = Dense(intermediate_dim[0])(inputs)\n",
    "    #h = BatchNormalization()(h) \n",
    "    #h = Dropout(0.1)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "    h = Dense(intermediate_dim[1])(h)\n",
    "    #h = BatchNormalization()(h)\n",
    "    #h = Dropout(0.1)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "    h = Dense(intermediate_dim[2])(h)\n",
    "    #h = BatchNormalization()(h)\n",
    "    #h = Dropout(0.1)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "   \n",
    "\n",
    "    cat_outputs = []\n",
    "    #i='Home_income'\n",
    "    #i=0\n",
    "    for i in ['Home_income', 'Home_car', 'Home_drive', 'Age', 'Gender','Home_type']:\n",
    "        t = Dense(x_train_cond_R[i].nunique())(h)\n",
    "        #t = Activation('softmax')(t)\n",
    "        t = gumbel(t,6)\n",
    "        cat_outputs.append(t)\n",
    "    \n",
    "    tp_outputs = []\n",
    "    p = Dense(48,activation='relu')(x)\n",
    "    p = Dense(24,activation='relu')(p)\n",
    "    p = Dense(12,activation='relu')(p)\n",
    "    for i in range(5):\n",
    "        t = Dense(6)(p[:,i,:])\n",
    "        #t = Activation('softmax')(t)\n",
    "        t = gumbel(t,6)\n",
    "        cat_outputs.append(t)\n",
    "                                 \n",
    "       \n",
    "    concat = Concatenate()(cat_outputs)\n",
    "    \n",
    "    \n",
    "    model = Model([noise,label_ns,label],concat)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic():\n",
    "    \n",
    "    img = Input(shape=x_train_cond.shape[1])\n",
    "    label = Input(shape=(maxlen,embed_dim))\n",
    "    label_ns = Input(shape=(nseq_dim))  \n",
    "    \n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(label)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    inputs = Concatenate()([img,label_ns,x])  \n",
    "\n",
    "    h = Dense(intermediate_dim[2])(inputs)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(intermediate_dim[1])(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(intermediate_dim[0])(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    validity = Dense(1)(h)\n",
    "    \n",
    "    model = Model(inputs = [img,label_ns,label],outputs = validity)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def RandomWeightedAverage(inputs):\n",
    "    alpha = K.random_uniform((BATCH_SIZE, 1))\n",
    "    return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting hyperparameters from MultiCATGAN\n",
    "intermediate_dim = [256,256,256]\n",
    "latent_dim = 128\n",
    "optimizer = Adam(lr=2e-04) ## \n",
    "BATCH_SIZE = 256\n",
    "gumbel = GumbelSoftmax(name = 'gumbel')\n",
    "embed_dim = num_features\n",
    "nseq_dim = y_train_nseq.shape[1]\n",
    "num_heads = 4\n",
    "ff_dim = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Build\n",
    "generator = build_generator()\n",
    "critic = build_critic()\n",
    "\n",
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the Critic\n",
    "#-------------------------------\n",
    "\n",
    "# Freeze generator's layers while training critic\n",
    "generator.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "# Image input (real sample)\n",
    "real_img = Input(shape=x_train_cond.shape[1])\n",
    "\n",
    "# Noise input\n",
    "z_disc = Input(shape=(latent_dim))\n",
    "# Generate image based of noise (fake sample) and add label to the input \n",
    "label = Input(shape=(maxlen,embed_dim))\n",
    "label_ns = Input(shape=(nseq_dim))  \n",
    "fake_img = generator([z_disc,label_ns,label])\n",
    "\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = critic([fake_img,label_ns,label])\n",
    "valid = critic([real_img,label_ns,label])\n",
    "\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage([real_img, fake_img])\n",
    "\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = critic([interpolated_img,label_ns,label])\n",
    "\n",
    "partial_gp_loss = partial(gradient_penalty_loss,averaged_samples=interpolated_img)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "critic_model = Model(inputs=[real_img,label_ns,label,z_disc], outputs=[valid, fake, validity_interpolated])\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                           wasserstein_loss,\n",
    "                           partial_gp_loss],\n",
    "                           optimizer=optimizer,\n",
    "                           loss_weights=[1, 1, 10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------\n",
    "\n",
    "# For the generator we freeze the critic's layers\n",
    "critic.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z_gen = Input(shape=(latent_dim))\n",
    "# add label to the input\n",
    "label = Input(shape=(maxlen,embed_dim))\n",
    "label_ns = Input(shape=(nseq_dim))  \n",
    "# Generate images based of noise\n",
    "img = generator([z_gen,label_ns,label])\n",
    "\n",
    "# Discriminator determines validity\n",
    "valid = critic([img,label_ns,label])\n",
    "\n",
    "# Defines generator model\n",
    "generator_model = Model([z_gen,label_ns,label], valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           [(None, 5, 52)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Functional)            (None, 64)           265780      input_24[0][0]                   \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Functional)            (None, 1)            236505      model_4[1][0]                    \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_25[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 502,285\n",
      "Trainable params: 265,780\n",
      "Non-trainable params: 236,505\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "## Train\n",
    "epochs = 40000 # 1 hours 7000\n",
    "sample_interval = 500\n",
    "n_critic = 5\n",
    "BATCH_SIZE = 256\n",
    "losslog = []\n",
    "\n",
    "# Load the dataset\n",
    "X_train = x_train_cond.values.astype(\"float32\")\n",
    "y_train = y_train_seq_2d\n",
    "y_train_ns = y_train_nseq.values.astype(\"float32\")\n",
    "y_train_SC = y_train_SC_seq_2d\n",
    "y_train_SC_ns = y_train_SC_nseq.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Adversarial ground truths\n",
    "valid = - np.ones(BATCH_SIZE)\n",
    "fake =  np.ones(BATCH_SIZE)\n",
    "dummy = np.zeros(BATCH_SIZE) # Dummy gt for gradient penalty\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(n_critic):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        \n",
    "        \n",
    "        idx = np.random.randint(0,X_train.shape[0],BATCH_SIZE)\n",
    "        imgs, labels, labels_ns = X_train[idx], y_train[idx], y_train_ns[idx]\n",
    "\n",
    "        # Sample generator input\n",
    "        noise = np.random.normal(0,1,[BATCH_SIZE,latent_dim]) \n",
    "        # Train the critic\n",
    "        d_loss = critic_model.train_on_batch([imgs, labels_ns,labels, noise], [valid, fake, dummy])\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "    idx_SC = np.random.randint(0, y_train.shape[0], BATCH_SIZE)\n",
    "    sampled_labels,sampled_labels_ns = y_train_SC[idx_SC],y_train_SC_ns[idx_SC]\n",
    "    g_loss = generator_model.train_on_batch([noise,sampled_labels_ns,sampled_labels], valid)\n",
    "\n",
    "    # Plot the progress\n",
    "    # Plot the progress\n",
    "\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if epoch % sample_interval == 0:\n",
    "        print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "        losslog.append([d_loss[0], g_loss])\n",
    "        generator.save_weights('Py_generator/AttnMO_XY_F1', overwrite=True)\n",
    "        critic.save_weights('Py_critic/AttnMO_XY_F1', overwrite=True)\n",
    "        \n",
    "        \n",
    "print(\"time :\", time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\euijin\\anaconda3\\envs\\c8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "def generate_images(label,label_ns):\n",
    "    generator.load_weights('Py_generator/AttnMO_XY_F1')\n",
    "    noise = np.random.normal(0, 1, (label.shape[0],latent_dim))\n",
    "    gen_imgs = generator.predict([noise,label_ns,label])\n",
    "   \n",
    "    \n",
    "    return gen_imgs\n",
    "\n",
    "\n",
    "from random import sample\n",
    "\n",
    "# SC case\n",
    "y_test_SC = selo(y_test_SC_seq,len(y_test_SC_seq))\n",
    "y_test_SC_ns = y_test_SC_nseq.values.astype(\"float32\")\n",
    "#idx = sample(range(y_test_SC.shape[0]),x_test_cond.shape[0])\n",
    "samples_act = y_test_SC\n",
    "samples_act_ns = y_test_SC_ns\n",
    "samples_pop_SC = generate_images(samples_act,samples_act_ns)\n",
    "\n",
    "# num_gen = 3\n",
    "# for i in range(num_gen):\n",
    "#     samples_pop_SC = np.concatenate((samples_pop_SC,generate_images(samples_act,samples_act_ns)),axis=0)\n",
    "\n",
    "\n",
    "## NHTS case\n",
    "y_test_SC = selo(y_test_seq,len(y_test_seq))\n",
    "y_test_SC_ns = y_test_nseq.values.astype(\"float32\")\n",
    "idx = sample(range(y_test_SC.shape[0]),x_test_cond.shape[0])\n",
    "\n",
    "x_test_cond.shape[0]\n",
    "\n",
    "samples_act = y_test_SC[idx,:]\n",
    "samples_act_ns = y_test_SC_ns[idx,:]\n",
    "samples_pop = generate_images(samples_act,samples_act_ns)\n",
    "\n",
    "\n",
    "\n",
    "## Make Ground Truth & Test\n",
    "n_uni_col = [x_train_cond_R[i].nunique() for i in x_train_cond_R.columns[1:7]]\n",
    "n_uni_col = [0]+n_uni_col+[6,6,6,6,6]\n",
    "n_uni_col = np.cumsum(n_uni_col)\n",
    "col_pop = x_test_cond.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_to_long(samples_pop):\n",
    "    resamples = []\n",
    "    for j in range(samples_pop.shape[0]):\n",
    "        if(type(samples_pop) is np.ndarray):\n",
    "            sam = samples_pop[j]\n",
    "        else:\n",
    "            sam = samples_pop.values[j]\n",
    "        resamples_row = []\n",
    "        for i in range(len(n_uni_col)-1):\n",
    "            idx = range(n_uni_col[i],n_uni_col[i+1])\n",
    "            resamples_row = np.append(resamples_row,np.random.choice(col_pop[idx],p=sam[idx],size=1))\n",
    "        resamples = np.concatenate((resamples,resamples_row),axis=0)\n",
    "    resamples = resamples.reshape(samples_pop.shape[0],len(n_uni_col)-1 )\n",
    "    resamples = pd.DataFrame(resamples,columns= x_train_cond_R.columns[1:7].to_list()+[\"TP_0\",\"TP_1\",\"TP_2\",\"TP_3\",\"TP_4\"])\n",
    "    resamples = resamples.apply(lambda x: x.astype('category'))\n",
    "    return(resamples)\n",
    "\n",
    "def mean_JSD(samples,resamples):\n",
    "    Marg_JSD = []\n",
    "    for col in samples.columns:\n",
    "        resam = pd.value_counts(resamples[col]).sort_index()\n",
    "        sam = pd.value_counts(samples[col]).sort_index()\n",
    "        tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "        tab = tab.fillna(0)\n",
    "        Marg_JSD.append(jensenshannon(tab.iloc[:,0], tab.iloc[:,1]))\n",
    "     \n",
    "\n",
    "    bi_index = combinations(samples.columns,2)\n",
    "    bi_index = list(bi_index)\n",
    "    col1,col2 = bi_index[0]\n",
    "\n",
    "    Bi_JSD = []\n",
    "    for col1,col2 in bi_index:\n",
    "        resam = pd.DataFrame(pd.crosstab(resamples[col1],resamples[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "        sam = pd.DataFrame(pd.crosstab(samples[col1],samples[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "        tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "        tab = tab.fillna(0)\n",
    "        Bi_JSD.append(jensenshannon(tab.iloc[:,0], tab.iloc[:,1]))\n",
    "\n",
    "    return([Marg_JSD,Bi_JSD])\n",
    "\n",
    "\n",
    "def get_resamples(y_test_SC_seq,y_test_SC_nseq_ns,num_gen=1):\n",
    "    resamples_SC = pd.DataFrame()\n",
    "    for i in range(num_gen):\n",
    "        y_test_SC = selo(y_test_SC_seq,len(y_test_SC_seq))\n",
    "        y_test_SC_ns = y_test_SC_nseq.values.astype(\"float32\")\n",
    "        idx = sample(range(y_test_SC.shape[0]),x_test_cond.shape[0])\n",
    "        samples_act = y_test_SC[idx,:]\n",
    "        samples_act_ns = y_test_SC_ns[idx,:]\n",
    "        samples_pop_SC = generate_images(samples_act,samples_act_ns)\n",
    "        resamples_SC = pd.concat([resamples_SC,wide_to_long(samples_pop_SC)],axis=0)\n",
    "    return(resamples_SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = wide_to_long(x_test_cond)\n",
    "resamples = wide_to_long(samples_pop)\n",
    "resamples_SC = wide_to_long(samples_pop_SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Preprocessing\n",
    "C1 = sum(samples['TP_0'] == 'TP_0_Z')\n",
    "C2 = sum((samples['TP_1'] == 'TP_1_Z'))\n",
    "C3 = sum((samples['TP_2'] == 'TP_2_Z') & ((samples['TP_3'] != 'TP_3_Z') | (samples['TP_4'] != 'TP_4_Z')))\n",
    "C4 = sum((samples['TP_3'] == 'TP_3_Z') & ((samples['TP_4'] != 'TP_4_Z')))\n",
    "CZ = C1+C2+C3+C4\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2)\n",
    "ax = np.reshape(ax,(-1))\n",
    "colors = ['red', 'lime']\n",
    "label=['Generated', 'Training']\n",
    "\n",
    "i=0\n",
    "for i in range(len(samples.columns[:-1])):\n",
    "    \n",
    "    resam = pd.value_counts(resamples.iloc[:,i]).sort_index()\n",
    "    sam = pd.value_counts(samples.iloc[:,i]).sort_index()\n",
    "    tab = pd.merge(resam,sam,left_index=True, right_index=True,how='right')\n",
    "    tab = tab.fillna(0)\n",
    "    ticks = np.arange(tab.shape[0])\n",
    "    w = 0.3\n",
    "    ax[i].bar(ticks-0.5*w,tab.iloc[:,0],align='center',width =  w,color=colors[0], label=label[0])\n",
    "    ax[i].bar(ticks+0.5*w,tab.iloc[:,1],align='center',width =  w,color=colors[1], label=label[1])\n",
    "    ax[i].set_title(samples.columns[i],fontsize=8)\n",
    "    ax[i].tick_params(axis='y', labelsize=5)\n",
    "    #ax[i].autoscale(tight=True)\n",
    "    ax[i].set_xticks(ticks)\n",
    "    ax[i].set_xticklabels(labels = tab.index.to_list(),fontsize=5,rotation=40)\n",
    "\n",
    "plt.subplots_adjust(left=0.15, right=0.85, top=0.90,bottom=0.10,wspace = 0.2,hspace=0.2)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,12)\n",
    "plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('line_plot_hq.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Visualize 2D attributes\n",
    "from scipy.stats import kde\n",
    "LU_TAZ = pd.read_csv(\"LU_TAZ.csv\")\n",
    "\n",
    "Gu_S = y_test_cond_SC_R[['P_Arrival_code','P_Arrival_x','P_Arrival_y']]\n",
    "Gu_S = Gu_S.drop_duplicates(subset='P_Arrival_code')\n",
    "Gu_S['Gu_code'] = Gu_S['P_Arrival_code'].astype(\"str\").str[:4]\n",
    "\n",
    "\n",
    "LU_S = LU_TAZ.merge(Gu_S, left_on='P_Arrival_code', right_on='P_Arrival_code')\n",
    "LU_S = LU_S[[\"P_Arrival_x_y\",\"P_Arrival_y_y\",\"P_Arr_LU1\",\"P_Arr_LU2\",\"P_Arr_LU3\"]]\n",
    "LU_S.columns = [\"X\",\"Y\",'LU1',\"LU2\",\"LU3\"]\n",
    "LU_S['LU1'] = pd.to_numeric(LU_S['LU1'])\n",
    "LU_S['LU2'] = pd.to_numeric(LU_S['LU2'])\n",
    "LU_S['LU3'] = pd.to_numeric(LU_S['LU3'])\n",
    "LU_S['X'] = pd.to_numeric(LU_S['X'])\n",
    "LU_S['Y'] = pd.to_numeric(LU_S['Y'])\n",
    "\n",
    "## NHTS_True\n",
    "# S_TP=np.concatenate([np.array(samples.iloc[:,6:11]).reshape((-1,1)),np.array(y_test_seq[:,:,52:54]).reshape((-1,1,2)).reshape((-1,2))],axis=1)\n",
    "# S_TP=pd.DataFrame(S_TP,columns = ['TP','X','Y'])\n",
    "# S_TP['TP'] = [S_TP['TP'][x][-1] for x in range(S_TP['TP'].shape[0])]\n",
    "\n",
    "# ## NHTS_Gen\n",
    "# S_TP=np.concatenate([np.array(resamples.iloc[:,6:11]).reshape((-1,1)),np.array(y_test_seq[:,:,52:54]).reshape((-1,1,2)).reshape((-1,2))],axis=1)\n",
    "# S_TP=pd.DataFrame(S_TP,columns = ['TP','X','Y'])\n",
    "# S_TP['TP'] = [S_TP['TP'][x][-1] for x in range(S_TP['TP'].shape[0])]\n",
    "\n",
    "# ## SC_Gen\n",
    "S_TP=np.concatenate([np.array(resamples_SC.iloc[:,6:11]).reshape((-1,1)),np.array(y_test_SC_seq[:,:,52:54]).reshape((-1,1,2)).reshape((-1,2))],axis=1)\n",
    "S_TP=pd.DataFrame(S_TP,columns = ['TP','X','Y'])\n",
    "S_TP['TP'] = [S_TP['TP'][x][-1] for x in range(S_TP['TP'].shape[0])]\n",
    "\n",
    "## Land_use\n",
    "# S_TP=np.concatenate([np.array(samples.iloc[:,6:11]).reshape((-1,1)),np.array(y_test_seq[:,:,52:54]).reshape((-1,1,2)).reshape((-1,2))],axis=1)\n",
    "# S_TP=pd.DataFrame(S_TP,columns = ['TP','X','Y'])\n",
    "# S_TP['TP'] = [S_TP['TP'][x][-1] for x in range(S_TP['TP'].shape[0])]\n",
    "\n",
    "\n",
    "\n",
    "S_TP_0 = S_TP[S_TP['TP']=='0'] \n",
    "S_TP_1 = S_TP[S_TP['TP']=='1'] \n",
    "S_TP_2 = S_TP[S_TP['TP']=='2'] \n",
    "S_TP_3 = S_TP[S_TP['TP']=='3'] \n",
    "S_TP_4 = S_TP[S_TP['TP']=='4'] \n",
    "\n",
    "LU_S_1 = LU_S[[\"X\",\"Y\",\"LU1\"]]\n",
    "LU_S_2 = LU_S[[\"X\",\"Y\",\"LU2\"]]\n",
    "LU_S_3 = LU_S[[\"X\",\"Y\",\"LU3\"]]\n",
    "\n",
    "S_CNT_0 = S_TP_0.groupby(['X','Y']).count()\n",
    "S_CNT_0 = pd.concat([S_CNT_0.index.to_frame(index=False),pd.DataFrame(S_CNT_0['TP'].values)],axis=1)\n",
    "S_CNT_1 = S_TP_1.groupby(['X','Y']).count()\n",
    "S_CNT_1 = pd.concat([S_CNT_1.index.to_frame(index=False),pd.DataFrame(S_CNT_1['TP'].values)],axis=1)\n",
    "S_CNT_2 = S_TP_2.groupby(['X','Y']).count()\n",
    "S_CNT_2 = pd.concat([S_CNT_2.index.to_frame(index=False),pd.DataFrame(S_CNT_2['TP'].values)],axis=1)\n",
    "S_CNT_3 = S_TP_3.groupby(['X','Y']).count()\n",
    "S_CNT_3 = pd.concat([S_CNT_3.index.to_frame(index=False),pd.DataFrame(S_CNT_3['TP'].values)],axis=1)\n",
    "S_CNT_4 = S_TP_4.groupby(['X','Y']).count()\n",
    "S_CNT_4 = pd.concat([S_CNT_4.index.to_frame(index=False),pd.DataFrame(S_CNT_4['TP'].values)],axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1,sharey=True)\n",
    "\n",
    "S_CNT_0.plot.hexbin(x='X', y='Y', gridsize=19,ax=axes[0],figsize=(20,20))\n",
    "S_CNT_2.plot.hexbin(x='X', y='Y', gridsize=19,ax=axes[1],figsize=(20,20))\n",
    "S_CNT_3.plot.hexbin(x='X', y='Y', gridsize=19,ax=axes[2],figsize=(20,20))\n",
    "S_CNT_4.plot.hexbin(x='X', y='Y', gridsize=19,ax=axes[3],figsize=(20,20))\n",
    "plt.show()\n",
    "#plt.savefig('C:/Users/euijin/Desktop/Interpretable_ML/05_Papers/DataFusion_2021/Spatial_TP_NHTS_True.svg', dpi=300)\n",
    "plt.savefig('C:/Users/euijin/Desktop/Interpretable_ML/05_Papers/DataFusion_2021/Spatial_TP_SC_Gen.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1,sharex=True,sharey=True)\n",
    "\n",
    "LU_S_1.plot.hexbin(x='X', y='Y',C='LU1', gridsize=19,ax=axes[0],figsize=(20,20))\n",
    "LU_S_2.plot.hexbin(x='X', y='Y',C='LU2', gridsize=19,ax=axes[1])\n",
    "LU_S_3.plot.hexbin(x='X', y='Y',C='LU3', gridsize=19,ax=axes[2])\n",
    "LU_S_3.plot.hexbin(x='X', y='Y',C='LU3', gridsize=19,ax=axes[3])\n",
    "plt.show()\n",
    "plt.savefig('C:/Users/euijin/Desktop/Interpretable_ML/05_Papers/DataFusion_2021/Spatial_LU.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4084a42afd5044598c16be71be7f4477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample data\n",
    "# ## SC_Gen\n",
    "T_TP=np.concatenate([np.array(resamples_SC.iloc[:,6:11]).reshape((-1,1)),np.array(y_test_SC_seq[:,:,21:41]).reshape((-1,20))],axis=1)\n",
    "ActicityDuration = np.argmax(T_TP[:,1:21],axis=1)\n",
    "T_TP=pd.DataFrame({'TP' : T_TP[:,0],'AD':ActicityDuration})\n",
    "T_TP['TP'] = [T_TP['TP'].iloc[x][-1] for x in range(T_TP['TP'].shape[0])]\n",
    "T_TP=T_TP[-(T_TP['TP'] == 'Z')]\n",
    "\n",
    "T_TP_CNT_0 = T_TP[T_TP['TP']=='0'].groupby('AD').count()\n",
    "T_TP_CNT_1 = T_TP[T_TP['TP']=='1'].groupby('AD').count()\n",
    "T_TP_CNT_2 = T_TP[T_TP['TP']=='2'].groupby('AD').count()\n",
    "T_TP_CNT_3 = T_TP[T_TP['TP']=='3'].groupby('AD').count()\n",
    "T_TP_CNT_4 = T_TP[T_TP['TP']=='4'].groupby('AD').count()\n",
    "\n",
    "T_TP_CNT = pd.concat([T_TP_CNT_0,T_TP_CNT_1,T_TP_CNT_2,T_TP_CNT_3,T_TP_CNT_4],axis=1)\n",
    "T_TP_CNT.columns = ['Commute','Work','Organized Hobby','Entertainment','Returning Home']\n",
    "T_TP_CNT = T_TP_CNT.loc[[9,0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19],:]\n",
    "T_TP_CNT.index = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "\n",
    "T_TP_CNT.plot(xticks = [0,2,4,6,8,10,12,14,16,18],style='o-')\n",
    "plt.show()\n",
    "plt.savefig('C:/Users/euijin/Desktop/Interpretable_ML/05_Papers/DataFusion_2021/Temporal_AD_SC.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b1211bb9e4b319ed621d70b6a800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample data\n",
    "# ## SC_Gen\n",
    "T_TP=np.concatenate([np.array(resamples_SC.iloc[:,6:11]).reshape((-1,1)),np.array(y_test_SC_seq[:,:,2:21]).reshape((-1,19))],axis=1)\n",
    "Arrival_Time = np.argmax(T_TP[:,1:20],axis=1)\n",
    "T_TP=pd.DataFrame({'TP' : T_TP[:,0],'AT':Arrival_Time})\n",
    "T_TP['TP'] = [T_TP['TP'].iloc[x][-1] for x in range(T_TP['TP'].shape[0])]\n",
    "T_TP=T_TP[-(T_TP['TP'] == 'Z')]\n",
    "\n",
    "T_TP_CNT_0 = T_TP[T_TP['TP']=='0'].groupby('AT').count()\n",
    "T_TP_CNT_1 = T_TP[T_TP['TP']=='1'].groupby('AT').count()\n",
    "T_TP_CNT_2 = T_TP[T_TP['TP']=='2'].groupby('AT').count()\n",
    "T_TP_CNT_3 = T_TP[T_TP['TP']=='3'].groupby('AT').count()\n",
    "T_TP_CNT_4 = T_TP[T_TP['TP']=='4'].groupby('AT').count()\n",
    "\n",
    "T_TP_CNT = pd.concat([T_TP_CNT_0,T_TP_CNT_1,T_TP_CNT_2,T_TP_CNT_3,T_TP_CNT_4],axis=1)\n",
    "T_TP_CNT.columns = ['Commute','Work','Organized Hobby','Entertainment','Returning Home']\n",
    "T_TP_CNT.index = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
    "\n",
    "\n",
    "T_TP_CNT.plot(xticks=[5,7,9,11,13,15,17,19,21,23],style='o-')\n",
    "plt.show()\n",
    "plt.savefig('C:/Users/euijin/Desktop/Interpretable_ML/05_Papers/DataFusion_2021/Temporal_Arr_SC.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
