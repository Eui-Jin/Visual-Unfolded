{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate, Layer, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, Flatten,LeakyReLU,ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from gumbel_softmax_EJ import GumbelSoftmax\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "pd.options.display.max_rows = 2000\n",
    "\n",
    "\"\"\"\n",
    "## Implement multi head self attention as a Keras layer\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "## Implement a Transformer block as a layer\n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\"\"\"\n",
    "# Positional Embedding\n",
    "\"\"\"\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding1D, self).__init__()\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 3:\n",
    "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
    "        _, x, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1)\n",
    "        emb = torch.zeros((x,self.channels),device=tensor.device).type(tensor.type())\n",
    "        emb[:,:self.channels] = emb_x\n",
    "\n",
    "        return emb[None,:,:orig_ch]\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        channels = int(np.ceil(channels/2))\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "        _, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1)\n",
    "        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1)\n",
    "        emb = torch.zeros((x,y,self.channels*2),device=tensor.device).type(tensor.type())\n",
    "        emb[:,:,:self.channels] = emb_x\n",
    "        emb[:,:,self.channels:2*self.channels] = emb_y\n",
    "        return emb[None,:,:,:orig_ch]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "gpu = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device = gpu, enable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Data processed by R\n",
    "x_train_cond_R = pd.read_csv('01_Data/x_train_cond_R_Coord_ID_DisR.csv')\n",
    "y_train_cond_R = pd.read_csv('01_Data/y_train_cond_R_Coord_ID_DisR.csv')\n",
    "y_test_cond_SC_R = pd.read_csv('01_Data/y_test_cond_SC_R_Coord_ID_DisR.csv')\n",
    "y_train_cond_SC_R = pd.read_csv('01_Data/y_train_cond_SC_R_Coord_ID_DisR.csv')\n",
    "\n",
    "x_train_cond_R = x_train_cond_R[y_train_cond_R['Max_seq'] != 6]\n",
    "y_train_cond_R = y_train_cond_R[y_train_cond_R['Max_seq'] != 6]\n",
    "y_test_cond_SC_R = y_test_cond_SC_R[y_test_cond_SC_R['Max_seq'] != 6]\n",
    "y_train_cond_SC_R = y_train_cond_SC_R[y_train_cond_SC_R['Max_seq'] != 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_catcol = x_train_cond_R.columns.drop(['ID','P_Trip_seq'])\n",
    "y_catcol = y_train_cond_R.columns.drop(['ID','P_Trip_seq',\"JIGA\",\"P_Home_Meanage\",\"P_Home_Older\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cond_R[x_catcol]= x_train_cond_R[x_catcol].apply(lambda x: x.astype('category') )\n",
    "y_train_cond_R[y_catcol]= y_train_cond_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "y_test_cond_SC_R[y_catcol]= y_test_cond_SC_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "y_train_cond_SC_R[y_catcol]= y_train_cond_SC_R[y_catcol].apply(lambda x: x.astype('category'))\n",
    "\n",
    "x_train_cond_R = x_train_cond_R.sort_values(by=['ID','P_Trip_seq'],axis=0)\n",
    "y_train_cond_R = y_train_cond_R.sort_values(by=['ID','P_Trip_seq'],axis=0)\n",
    "y_train_cond_SC_R = y_train_cond_SC_R.sort_values(by=['NID','P_Trip_seq'],axis=0)\n",
    "y_test_cond_SC_R = y_test_cond_SC_R.sort_values(by=['NID','P_Trip_seq'],axis=0)\n",
    "\n",
    "\n",
    "## Ground-truth of X\n",
    "\n",
    "samples = pd.concat([x_train_cond_R['ID'],x_train_cond_R[x_catcol].drop('P_Trip_purpose',axis=1)],axis=1)\n",
    "samples_R = x_train_cond_R.copy()\n",
    "samples_R['idx'] = samples_R.groupby('ID').cumcount()\n",
    "samples_R['prod_idx'] = 'TP_' + samples_R.idx.astype(str)\n",
    "\n",
    "Trip_purpose = samples_R.pivot(index='ID',columns='prod_idx',values='P_Trip_purpose')\n",
    "for col in Trip_purpose.columns:\n",
    "    Trip_purpose[col] = Trip_purpose[col].cat.add_categories(\"Z\").fillna(\"Z\")    \n",
    "samples =  pd.merge(samples.groupby('ID').head(1),Trip_purpose,on=\"ID\")\n",
    "\n",
    "\n",
    "## Processing the x_train_cond\n",
    "x_train_cond = pd.concat([x_train_cond_R['ID'],pd.get_dummies(x_train_cond_R[x_catcol].drop('P_Trip_purpose',axis=1))],axis=1)\n",
    "x_train_cond_R['idx'] = x_train_cond_R.groupby('ID').cumcount()\n",
    "x_train_cond_R['prod_idx'] = 'TP_' + x_train_cond_R.idx.astype(str)\n",
    "\n",
    "Trip_purpose = x_train_cond_R.pivot(index='ID',columns='prod_idx',values='P_Trip_purpose')\n",
    "for col in Trip_purpose.columns:\n",
    "    Trip_purpose[col] = Trip_purpose[col].cat.add_categories(\"Z\").fillna(\"Z\")\n",
    "    \n",
    "Trip_purpose = Trip_purpose[Trip_purpose['TP_1'] != 'Z']\n",
    "\n",
    "Trip_purpose = pd.get_dummies(Trip_purpose)\n",
    "x_train_cond =  pd.merge(x_train_cond.groupby('ID').head(1),Trip_purpose,on=\"ID\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = y_train_cond_R[['isHome','P_Arrival_time','stay_time','tr_time']]\n",
    "#y_train_num = y_train_cond_R[['Age_SC','start_time','stay_time','tr_time','JIGA','P_Home_Meanage','P_Home_Older']]\n",
    "y_train_seq = pd.concat([pd.get_dummies(y_train_cat),y_train_cond_R[['P_Arrival_x','P_Arrival_y','ID','P_Trip_seq']]],axis=1)\n",
    "y_train_seq = y_train_seq[y_train_seq['ID'].isin(x_train_cond['ID'])]\n",
    "\n",
    "\n",
    "y_train_SC_cat = y_train_cond_SC_R[['isHome','P_Arrival_time','stay_time','tr_time']]\n",
    "y_train_SC_seq = pd.concat([pd.get_dummies(y_train_SC_cat),y_train_cond_SC_R[['P_Arrival_x','P_Arrival_y','NID','P_Trip_seq']]],axis=1)\n",
    "\n",
    "y_train_nseq = pd.concat([pd.get_dummies(y_train_cond_R[['Age_SC','start_time']]),y_train_cond_R[['ID','JIGA','P_Home_Meanage','P_Home_Older']]],axis=1)\n",
    "y_train_SC_nseq = pd.concat([pd.get_dummies(y_train_cond_SC_R[['Age_SC','start_time']]),y_train_cond_SC_R[['NID','JIGA','P_Home_Meanage','P_Home_Older']]],axis=1)\n",
    "\n",
    "y_train_nseq = y_train_nseq.groupby('ID').head(1)\n",
    "y_train_nseq = y_train_nseq[y_train_nseq['ID'].isin(x_train_cond['ID'])]\n",
    "y_train_nseq = y_train_nseq.drop(['ID'],axis=1)\n",
    "\n",
    "y_train_SC_nseq = y_train_SC_nseq.groupby('NID').head(1)\n",
    "y_train_SC_nseq = y_train_SC_nseq.drop(['NID'],axis=1)\n",
    "\n",
    "x_train_cond = x_train_cond.drop(['ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(y_train_seq.columns)-4\n",
    "maxlen = 5\n",
    "num_data = y_train_seq['ID'].nunique()\n",
    "num_data_SC = y_train_SC_seq['NID'].nunique()\n",
    "\n",
    "## Adding dummy dimension to be divded 4\n",
    "for i in range(3):\n",
    "    y_train_seq.insert(num_features,i,0)\n",
    "    y_train_SC_seq.insert(num_features,i,0)\n",
    "\n",
    "num_features = len(y_train_seq.columns)-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero padding\n",
    "def pad(x):\n",
    "    zero_data = np.zeros(shape=(maxlen - len(x),num_features+4))\n",
    "    d = pd.DataFrame(zero_data, columns=x.columns)\n",
    "    data = x.append(d, ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_SC_seq=y_train_SC_seq.groupby('NID').apply(pad)\n",
    "y_train_SC_seq=y_train_SC_seq.to_numpy()\n",
    "y_train_SC_seq=y_train_SC_seq.reshape(num_data_SC,maxlen,num_features+4)\n",
    "\n",
    "y_train_seq=y_train_seq.groupby('ID').apply(pad)\n",
    "y_train_seq=y_train_seq.to_numpy()\n",
    "y_train_seq=y_train_seq.reshape(num_data,maxlen,num_features+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def choice_train_test_split(X, y, y_ns, test_size=0.2,random_state=1004):\n",
    "    test_num = int(X.shape[0] * test_size)\n",
    "    train_num = X.shape[0] - test_num\n",
    "    np.random.seed(random_state)\n",
    "    train_idx = np.random.choice(X.shape[0], train_num, replace=False)\n",
    "    test_idx = np.setdiff1d(range(X.shape[0]), train_idx)\n",
    "    X_train = X.iloc[train_idx, :]\n",
    "    X_test = X.iloc[test_idx, :]\n",
    "    y_train = y[train_idx,:]\n",
    "    y_test = y[test_idx,:]\n",
    "    y_train_ns = y_ns.iloc[train_idx,:]\n",
    "    y_test_ns = y_ns.iloc[test_idx,:]     \n",
    "\n",
    "    return X_train, X_test, y_train, y_test,y_train_ns,y_test_ns\n",
    " \n",
    "X_train, X_test, y_train, y_test, y_train_ns, y_test_ns = choice_train_test_split(x_train_cond,y_train_seq,y_train_nseq,test_size=0.2,random_state=1004)\n",
    "y_train_SC, y_test_SC,y_train_SC_ns,y_test_SC_ns = train_test_split(y_train_SC_seq,y_train_SC_nseq,test_size=0.2,shuffle=True,random_state=1004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('01_Data/y_train_seq', y_train,allow_pickle=True)\n",
    "np.save('01_Data/y_test_seq', y_test,allow_pickle=True)\n",
    "np.save('01_Data/y_train_SC_seq', y_train_SC,allow_pickle=True)\n",
    "np.save('01_Data/y_test_SC_seq', y_test_SC,allow_pickle=True)\n",
    "\n",
    "X_train.to_csv('01_Data/x_train_cond.csv',index=False)\n",
    "X_test.to_csv('01_Data/x_test_cond.csv',index=False)\n",
    "y_train_ns.to_csv('01_Data/y_train_nseq.csv',index=False)\n",
    "y_test_ns.to_csv('01_Data/y_test_nseq.csv',index=False)\n",
    "y_train_SC_ns.to_csv('01_Data/y_train_SC_nseq.csv',index=False)\n",
    "y_test_SC_ns.to_csv('01_Data/y_test_SC_nseq.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
